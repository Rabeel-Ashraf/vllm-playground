# vLLM CPU Container for RHEL 9 / vllm-playground
# Modified for RHEL 9 / Podman compatibility:
#   - Removed --mount=type=cache directives (causes /tmp permission issues on RHEL/Podman)
#   - Single-stage final image (avoids COPY --from xattr issues in rootless Podman)
#   - Build artifacts cleaned up to minimize image size
# Optimized for production deployment with vllm-playground
#
# Supported platforms:
#   - linux/amd64 (x86_64)
#   - linux/arm64 (aarch64)
#
# Build arguments:
#   max_jobs=4 (default) - Number of parallel compile jobs (lower = more stable)
#   PYTHON_VERSION=3.12 (default)|3.11|3.10
#   VLLM_CPU_DISABLE_AVX512=false (default)|true - Disable AVX512 for older CPUs
#   VLLM_CPU_AVX512BF16=false (default)|true
#   VLLM_CPU_AVX512VNNI=false (default)|true
#   VLLM_CPU_AMXBF16=false (default)|true
#
# Build command:
#   podman build -f Containerfile.cpu -t vllm-cpu:latest .
#   podman build -f Containerfile.cpu --build-arg max_jobs=2 -t vllm-cpu:latest .

######################### BASE IMAGE (Common) #########################
FROM ubuntu:22.04 AS base-common

WORKDIR /workspace/

ARG PYTHON_VERSION=3.12
ARG PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"

# Install system dependencies
# Note: Removed --mount=type=cache for apt directories due to Podman/RHEL compatibility issues
# (causes "Couldn't create temporary file /tmp/apt.conf" errors on some systems)
RUN apt-get update -y \
    && apt-get install -y --no-install-recommends \
        sudo ccache git curl wget ca-certificates \
        gcc-12 g++-12 libtcmalloc-minimal4 libnuma-dev \
        ffmpeg libsm6 libxext6 libgl1 jq lsof \
    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 \
    && curl -LsSf https://astral.sh/uv/install.sh | sh \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Compiler configuration
ENV CC=/usr/bin/gcc-12 CXX=/usr/bin/g++-12
ENV CCACHE_DIR=/root/.cache/ccache
ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache

# Python environment setup
ENV PATH="/root/.local/bin:$PATH"
ENV VIRTUAL_ENV="/opt/venv"
ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python
RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# UV package manager configuration
ENV UV_HTTP_TIMEOUT=500
ENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_INDEX_STRATEGY="unsafe-best-match"
ENV UV_LINK_MODE="copy"

# Install Python dependencies
COPY requirements/ /tmp/requirements/
RUN uv pip install --upgrade pip && \
    uv pip install -r /tmp/requirements/cpu.txt && \
    rm -rf /tmp/requirements

ARG TARGETARCH
ENV TARGETARCH=${TARGETARCH}

######################### BASE IMAGE (x86_64) #########################
FROM base-common AS base-amd64
ENV LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/opt/venv/lib/libiomp5.so"

######################### BASE IMAGE (arm64) #########################
FROM base-common AS base-arm64
ENV LD_PRELOAD="/usr/lib/aarch64-linux-gnu/libtcmalloc_minimal.so.4"

######################### BASE IMAGE (Architecture-specific) #########################
FROM base-${TARGETARCH} AS base
RUN echo 'ulimit -c 0' >> ~/.bashrc

######################### BUILD STAGE #########################
FROM base AS vllm-build

# Build configuration
ARG max_jobs=8
ENV MAX_JOBS=${max_jobs}

ARG GIT_REPO_CHECK=0
ARG VLLM_CPU_DISABLE_AVX512=0
ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}

ARG VLLM_CPU_AVX512BF16=0
ENV VLLM_CPU_AVX512BF16=${VLLM_CPU_AVX512BF16}

ARG VLLM_CPU_AVX512VNNI=0
ENV VLLM_CPU_AVX512VNNI=${VLLM_CPU_AVX512VNNI}

ARG VLLM_CPU_AMXBF16=0
ENV VLLM_CPU_AMXBF16=${VLLM_CPU_AMXBF16}

WORKDIR /workspace/vllm

# Install build dependencies
COPY requirements/ requirements/
RUN uv pip install -r requirements/cpu-build.txt

# Copy source code (including .git for version detection)
COPY . .

# Optional repository check
RUN if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh ; fi

# Display build configuration
RUN echo "=== vLLM Build Configuration ===" && \
    echo "Python: $(python3 --version)" && \
    echo "GCC: $(gcc --version | head -1)" && \
    echo "CPU: $(lscpu | grep 'Model name' || echo 'N/A')" && \
    echo "Cores: $(nproc)" && \
    echo "MAX_JOBS: $MAX_JOBS" && \
    echo "VLLM_TARGET_DEVICE: cpu" && \
    echo "================================"

# Build vLLM wheel for CPU
RUN VLLM_TARGET_DEVICE=cpu \
    CMAKE_BUILD_TYPE=Release \
    VERBOSE=1 \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38

# Install vLLM immediately after building (avoids multi-stage COPY xattr issues)
RUN uv pip install dist/*.whl

# Clean up build artifacts to reduce image size
RUN rm -rf /workspace/vllm/build /workspace/vllm/dist /workspace/vllm/.eggs \
    && rm -rf /root/.cache/ccache /root/.cache/uv \
    && find /workspace/vllm -name "*.o" -delete \
    && find /workspace/vllm -name "*.a" -delete

######################### PRODUCTION IMAGE #########################
# Note: Using vllm-build as final stage to avoid Podman xattr issues with COPY --from
FROM vllm-build AS vllm-openai

WORKDIR /workspace/

# Create entrypoint script that passes through CLI arguments (like official vLLM image)
COPY --chmod=755 <<'SCRIPT' /workspace/entrypoint.sh
#!/bin/bash
set -e

# Force CPU mode (critical for CPU-only deployments)
export VLLM_TARGET_DEVICE=${VLLM_TARGET_DEVICE:-cpu}
export VLLM_PLATFORM=${VLLM_PLATFORM:-cpu}
export CUDA_VISIBLE_DEVICES=""

# CPU performance tuning
export VLLM_CPU_KVCACHE_SPACE=${VLLM_CPU_KVCACHE_SPACE:-4}
export VLLM_CPU_OMP_THREADS_BIND=${VLLM_CPU_OMP_THREADS_BIND:-auto}

# HuggingFace authentication (for gated models)
[ -n "$HF_TOKEN" ] && export HUGGING_FACE_HUB_TOKEN=$HF_TOKEN

# Cache directories
export HF_HOME=${HF_HOME:-/root/.cache/huggingface}
export HUGGINGFACE_HUB_CACHE=${HUGGINGFACE_HUB_CACHE:-/root/.cache/huggingface/hub}

echo "=========================================="
echo "vLLM CPU Service Container"
echo "=========================================="
echo "vLLM version: $(python3 -c 'import vllm; print(vllm.__version__)' 2>/dev/null || echo 'unknown')"
echo "Platform: $VLLM_PLATFORM"
echo "Device: $VLLM_TARGET_DEVICE"
echo "CPU KV Cache Space: $VLLM_CPU_KVCACHE_SPACE GB"
[ -n "$HF_TOKEN" ] && echo "HuggingFace Token: configured"
echo "=========================================="
echo ""
echo "Starting vLLM with arguments: $@"
echo ""

# Execute vLLM with all passed CLI arguments
exec python3 -m vllm.entrypoints.openai.api_server "$@"
SCRIPT

# Expose vLLM API port
EXPOSE 8000

# Health check - verify vLLM API is responding
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8000/health || curl -f http://localhost:8000/v1/models || exit 1

# ENTRYPOINT runs entrypoint script which passes all arguments to vLLM
ENTRYPOINT ["/workspace/entrypoint.sh"]

# Default CMD - can be completely overridden by docker/podman run arguments
CMD ["--model", "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "--host", "0.0.0.0", "--port", "8000", "--dtype", "bfloat16"]
